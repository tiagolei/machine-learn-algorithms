# -*- coding: utf-8 -*-
"""trabalho_final_machine_learning_versao_final .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v3Ubuj8VhOnCyDWt40fwkGJkkbV91Tut

# *Pós-graduação em Ciência e Dados e Machine Learning*
### *Trabalho Final - Machine Learning*

**Disciplina:** Fundamentos de Machine Learning

**Professor:** André Juan Costa Vieira

**Turma:** ÚNICA

**Nomes dos Integrantes:**

1- Eugênia Cornils

2- Mariana Barros

3- Tiago Leite

## Qualidade de vinhos.

Você foi contratado como cientista de dados pela famosa vinícola **"Vini Tradizionali di Manduria"** para analisar todos os aspectos dos vinhos produzidos. Diversas questões foram levantadas, como: Compreender os padrões das características que proporcionam boas safras e a qualidade de novos vinhos produzidos, antes que a comunidade mundial **"Vins Spectaculaires"** os deguste, apontar quais são os novos **"blends"** que podem ter continuidade no desenvolvimento, dentre várias outras atividades que visam as boas tomadas de decisões, sempre com o intuito de servir os melhores rótulos, aumentando os lucros e diminuindo os gastos.        

A equipe de enólogos faz estudos frequentes para verificar as características de cada vinho, colocando-os em planilhas. Para melhor compreensão dos dados, descreveram o que significado de cada propriedade.   


##### Descrição

**0. Color:** Se o vinho é tinto vermelho ou branco

**1. Fixed Acidity:** Qtd.de Ácido não volátil, aquele que não evapora fácil

**2. Volatile Acidity:** Teor de ácido acético que leva a um sabor desagradável de vinagre

**3. Citric Acid:** Um tipo de ácido que age como conservante para aumentar o nível de acidez em pequena quantidade para adicionar aroma e sabor

**4. Residual Sugar:** Qtd. de açúcar restante depois da fermentação, mais de 45g/litro é doce

**5. Chlorides:** Qtd. de sal

**6. Free Sulfur Dioxide:** Componente que impede crescimento microbiano e a oxidação do vinho

**7. Total Sulfur Dioxide:** Qtd. de SO2 (dióxido de enxofre)

**8. Density:** Densidade do vinho,

**9. pH:** Nível de acidez ou potencial hidrogeniônico

**10. Sulphates:** Um adicional que contribui para níveis de SO2 e é
antimicróbico e antioxidante

**11. Alcohol:** Qtd. de álcool

**12. Qualidade:** Notas de 3 a 9

# Questões

### Importe todas as bibliotecas necessárias na célula abaixo
##### Organize-as de forma crescente em relação ao tamanho da frase
"""

!pip install lightgbm xgboost catboost

!pip install plotly

!pip install pyod

import random
import numpy as np
import pandas as pd
import seaborn as sns
import xgboost as xgb
from time import time
import warnings as warn
from sklearn import svm
import plotly.express as px
from sklearn import datasets
from matplotlib import style
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from xgboost import plot_importance
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from catboost import CatBoostClassifier
from pyod.models.iforest import IForest
from imblearn.over_sampling import SMOTE
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.pipeline import make_pipeline
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA as skPCA
from imblearn.under_sampling import TomekLinks
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import AdaBoostClassifier
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.metrics import classification_report
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_validate, cross_val_score
warn.filterwarnings('ignore')


from imblearn.combine import SMOTETomek
from imblearn.under_sampling import TomekLinks
from sklearn.preprocessing import MinMaxScaler
from sklearn.datasets import make_classification
from catboost import CatBoostClassifier



from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer

from google.colab import drive
drive.mount('/content/drive')

"""### Tratamento dos Dados


Sir. Pounce, enólogo de longa data, especializado em vinhos italianos, descobriu que estão faltando alguns valores nas planilhas, e que outros dados foram alterados pelo ex-funcionário Smeagle, dispensado por degustar vinhos 'preciosos'.   

**Utilize o dataset 'wines_preprocessing.csv' para fazer as questões abaixo.**

1- Busque os valores faltantes no dataset e trate-os.

2- Busque valores incongruentes no dataset, imprima e os trate.

3- Valide seus tratamentos com o dataset **'wines.csv'**, demonstrando se foi possível manter as distribuições de forma adequada.
"""

#importar a base de dados wines_pre_processing
wines = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines_pre_processing.csv')

#importar a base de dados wines
wines2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines.csv')

#observar o data frame com 100 linhas
wines.head(100)

wines.info()

#transforma dados que são object em numéricos
wine= wines['color']
for col in wines.columns:
  wines[col] = pd.to_numeric(wines[col], errors='coerce')
wines['color']= wine

wines.info()

#observa quantos valores de color tem de cada tipo, são identificados problemas nos dados
wines['color'].value_counts()

#é definida uma condição das cores dos vinhos serem somente brancos ou tintos
condition = (wines['color'] == 'white') | (wines['color'] == 'red')

# Fazer drop das linhas que não atendem à condição
wines = wines[condition]

wines.describe()

#São identificados dados nulos
(wines.isnull().sum())/len(wines)*100

#como são poucos valores nulos eles serão apagados
win = wines.dropna()

#é feita a divisão de color em duas colunas
win = pd.get_dummies(win, columns=['color'])

"""Analisar onde colocar esses gráficos"""

import pandas as pd
import matplotlib.pyplot as plt

# Assuming 'wines' and 'wines2' have the same structure and column names
columns_to_compare = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
                       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide',
                       'density', 'pH', 'sulphates', 'alcohol', 'quality']

# Plot histograms for each column
for column in columns_to_compare:
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.hist(wines[column], bins=30, color='blue', alpha=0.7, label='wines')
    plt.title(f'Distribution of {column} in wines')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.hist(wines2[column], bins=30, color='orange', alpha=0.7, label='wines2')
    plt.title(f'Distribution of {column} in wines2')
    plt.legend()

    plt.tight_layout()
    plt.show()

"""O dono da vinícola, Sir. Donald Shelby, tem um filho cursando especialização em ciência de dados, que, ao deparar-se com o dataset, pediu a você que, ao invés de ficar tratando dados com 'essas' técnicas triviais, fizesse um algoritmo de regressão logística para encontrar os valores faltantes na coluna 'Alcohol'. O Sr. Shelby é um homem conhecido como mafioso e considera seu filho um gênio, por isso, você, com fortes receios de sofrer consequências inusitadas por ordem do Don Corleone da atualidade, acatou o "pedido".


4- Desenvolver um algoritmo de regressão logística visando preencher os dados faltantes da coluna "Alcohol". Em seguida, valide os resultados com o dataset **"wines.csv"**, apresentando todas as métricas de classificação estudadas.

5- Você, ao ver os resultados encontrados, se adiantou e fez um modelo de regressão polinomial para dirimir a questão. Em seguida, validou os resultados com o dataset **"wines.csv"**, utilizando todas as métricas de regressão estudadas. Por fim, escreverá um e-mail explicando o motivo <u>técnico</u> que o levou a não utilizar a regressão logística neste problema, bem como qual a melhor técnica que encontrou para tratar os valores faltantes.
"""

wines3 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines.csv')
wine= wines3['color']
for col in wines3.columns:
  wines3[col] = pd.to_numeric(wines3[col], errors='coerce')
wines3['color']= wine
win3 = pd.get_dummies(wines3, columns=['color'])
indices_nulos = wines[wines['alcohol'].isnull()].index
linhas_correspondentes_df2 = wines.loc[indices_nulos]
scaler = MinMaxScaler()
win3 = win3.loc[indices_nulos]
X_alcool_comp = win3.drop('alcohol',axis=1)
y_alcool_comp = win3.loc[:,'alcohol']
numeric_columns = X_alcool_comp.select_dtypes(include=['float64', 'int64']).columns
win3[numeric_columns] = scaler.fit_transform(win3[numeric_columns])

wines = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines_pre_processing.csv')
condition = (wines['color'] == 'white') | (wines['color'] == 'red')
wines = wines[condition]
wine= wines['color']
for col in wines.columns:
  wines[col] = pd.to_numeric(wines[col], errors='coerce')
wines['color']= wine
win = pd.get_dummies(wines, columns=['color'])
win7 = win
win = win.dropna()
wines = pd.get_dummies(wines, columns=['color'])

win7

win['categoria_alcool'] = pd.cut(win['alcohol'], bins=[0, 10, 20], labels=['baixo', 'alto'], right=False)
win7['categoria_alcool'] = pd.cut(win7['alcohol'], bins=[0, 10, 20], labels=['baixo', 'alto'], right=False)

X = win.drop(['alcohol', 'categoria_alcool'], axis=1)

#realizar a normalizaçãoo utilizando o MinMax
numeric_columns = X.select_dtypes(include=['float64', 'int64']).columns
# Cria uma instância do MinMaxScaler
scaler = MinMaxScaler()
# Aplica a transformação às colunas numéricas
win[numeric_columns] = scaler.fit_transform(win[numeric_columns])
win7[numeric_columns] = scaler.fit_transform(win7[numeric_columns])

"""#Regressão Logística"""

# Criar 'categoria_alcool' apenas para os valores faltantes
win7['categoria_alcool'] = pd.cut(win['alcohol'], bins=[-float('inf'), 10, float('inf')], labels=['baixo', 'alto'], right=False)

# Dividir o DataFrame em dois: um com valores conhecidos e outro com valores faltantes
df_known = win7.dropna(subset=['categoria_alcool'])
df_unknown = win7[win7['categoria_alcool'].isnull()]

# Verificar se há dados suficientes para treinar o modelo
if len(df_known) > 1:  # Garantir pelo menos duas amostras
    # Dividir os dados conhecidos em recursos (X) e rótulos (y)
    X = df_known.drop(['alcohol', 'categoria_alcool'], axis=1)  # Excluir 'alcohol' e 'categoria_alcool' dos recursos
    y = df_known['categoria_alcool']

    # Imputar valores ausentes em X
    imputer = SimpleImputer(strategy='mean')
    X_imputed = imputer.fit_transform(X)

    # Criar modelo de regressão logística
    model = LogisticRegression(max_iter=1000)

    # Treinar o modelo
    model.fit(X_imputed, y)

    # Imputar valores ausentes em X_unknown
    X_unknown = df_unknown.drop(['alcohol', 'categoria_alcool'], axis=1)
    X_unknown_imputed = imputer.transform(X_unknown)

    # Prever valores faltantes na coluna 'categoria_alcool'
    predicted_categories = model.predict(X_unknown_imputed)

    # Preencher os valores faltantes na coluna 'categoria_alcool'
    win7.loc[win7['categoria_alcool'].isnull(), 'categoria_alcool'] = predicted_categories

    # # Preencher os valores de 'alcohol' com base na classificação em 'categoria_alcool'
    # win['alcohol'] = win.apply(lambda row: row['alcohol'] if pd.notnull(row['alcohol']) else 10.0 if row['categoria_alcool'] == 'baixo' else 15.0, axis=1)

    # Preencher os valores de 'alcohol' com base na classificação em 'categoria_alcool'
    win7['alcohol'] = win7.apply(lambda row: row['alcohol'] if pd.notnull(row['alcohol']) else np.random.uniform(8, 10) if row['categoria_alcool'] == 'baixo' else np.random.uniform(10, 15), axis=1)
    # Exibir o DataFrame resultante
    print(win7)
else:
    print("Não há dados suficientes para treinar o modelo.")

mse = mean_squared_error(win3['alcohol'].loc[indices_nulos], win7['alcohol'].loc[indices_nulos])
print(f'Mean Squared Error: {mse}')

"""# Regressão Linear"""

# columns_to_scale = win.columns[:-3]  # Excluindo as três últimas colunas
# scaler = StandardScaler()
# win[columns_to_scale] = scaler.fit_transform(win[columns_to_scale])

win = win.drop('Unnamed: 0',axis=1)

X_alcool = win.drop({'alcohol','categoria_alcool'},axis=1)
y_alcool = win.loc[:,'alcohol']

X_train, X_test, y_train, y_test = train_test_split(X_alcool,y_alcool ,test_size=0.30, random_state=101)

(X_train.isnull().sum())/len(win)*100

lmodel = LinearRegression()
lmodel.fit(X_train,y_train)

y_pred = lmodel.predict(X_test)

mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error: {mse}')

degree = 2
model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
model.fit(X_train, y_train)

# Faça previsões no conjunto de teste
y_pred = model.predict(X_test)

# Avalie o desempenho do modelo
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

y_pred_faltantes = model.predict(X_alcool_comp)

mse = mean_squared_error(y_alcool_comp, y_pred_faltantes)
print(f'Mean Squared Error: {mse}')

MAPE = np.mean(np.abs((y_alcool_comp - y_pred_faltantes) / y_alcool_comp)) * 100

print(f"Valor do MAPE dado em percentual: {MAPE}")

dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Parâmetros do modelo
params = {
    'objective': 'reg:squarederror',  # Objetivo para regressão
    'max_depth': 7,                    # Profundidade máxima da árvore
    'learning_rate': 0.1,              # Taxa de aprendizado
    'n_estimators': 800                # Número de árvores
}

# Treine o modelo XGBoost
model = xgb.train(params, dtrain, num_boost_round=100)

# Faça previsões no conjunto de teste
y_pred = model.predict(dtest)

# Avalie o desempenho do modelo
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
MAPE = np.mean(np.abs((y_alcool_comp - y_pred_faltantes) / y_alcool_comp)) * 100

print(f"Valor do MAPE dado em percentual: {MAPE}")

dcompara = xgb.DMatrix(X_alcool_comp, label=y_pred_faltantes)
y_pred_faltantes = model.predict(dcompara)
mse = mean_squared_error(y_alcool_comp, y_pred_faltantes)
print(f'Mean Squared Error: {mse}')
MAPE = np.mean(np.abs((y_alcool_comp - y_pred_faltantes) / y_alcool_comp)) * 100

print(f"Valor do MAPE dado em percentual: {MAPE}")

y_pred_faltantes

mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
MAPE = np.mean(np.abs((y_alcool_comp - y_pred_faltantes) / y_alcool_comp)) * 100

print(f"Valor do MAPE dado em percentual: {MAPE}")

"""**Prezado Sir. Donald Shelby,**

---



Conforme a solicitação do seu filho foi realizada a regressão logística para prever o valor de alcohol, a princípio me causou um pouco de estranhamento essa escolha, pois em geral essa técnica é utilizada para prever valores binários, onde em geral o objetivo é identificar a probabilidade de pertencer a alguma classe.
Porém, estudando um pouco mais observei que pode ser utilizada a técnica de categorização, assim gerei com os dados uma nova coluna chamada categoria_alcool em que o alcool era avaliado com baixo teor de álcool e alto teor de álcool.Depois disso foi realizada uma regressão logística nos dados já escalonados da base de dados para prever se a categoria de álcool seria baixa ou alta, e com o resultado da predição da regressão foram lançados valores aleatórios para o álcool entre seus limites de classificação.
O erro quadrático médio quando comparamos os valores faltantes da base de dados em processamento com a já processada  foi de 1,8 na regressão logística, 0,14 na regressão liear, 0,11 na regressão Polinomial e 0,09 utilizando o modelo XGBosst.
Assim a melhor escolha será a utilização do modelo XGBoost.

Atenciosamente,
Equipe formada por
1- Eugênia Cornils
2- Mariana
3- Tiago Leite

### Análise Exploratória
**Utilize o dataset 'wines.csv'**

A enóloga Marilyn Monroe, direta do Sir. Pounce, tomou conhecimento de suas habilidades exploratórias e requereu gráficos "chiques, rebuscados, enfeitados e nada triviais" que mostrassem, de forma interativa todos os dados e seus respectivos insights. A principal exigência é de que as paletas de cores sejam harmônicas, de modo que possam ser utilizadas em apresentações. Para isso, sugeriu a documentação a seguir:
[Colors Palettes](https://plotly.com/python/builtin-colorscales/)

1- Utilize um countplot para averiguar a quantidade de vinhos por cada avaliação de qualidade.
Separare entre vinhos tintos e brancos, fazendo um gráfico para cada tipo.

2- Utilize um jointplot para descrever a relação entre álcool e açucar. Utilizar o tipo 'KDE'.

3- Utilize um boxplot para verificar se existe algum vinho que seja considerado um outlier. Utilize **x = 'quality'** e **y='residual sugar'**. Identificando os outliers, crie um novo dataframe, utilize um barplot para contabilizar a quantidade de vinhos tintos
e brancos por qualidade de modo que as barras estejam sobrepostas em relação ao tipo de vinho.   

4- Faça um gráfico de correlação e encontre quais são as 'features' que contém correlações
positivas e negativas fortes  entre si. Em seguida, utilize o scatterplot, colocando no eixo "x" e "y"
cada variável correlata e descreva por escrito o motivo da distribuição e o sentido vetorial estarem apresentados
das respectivas formas.

**ps**: Para este problema, entenda como correlações fortes valores menores que -0.4 e maiores que 0.4.
"""

wines4=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines.csv')
mappings={'white':0,'red':1}
wines4['color']=wines4['color'].map(mappings)

import plotly.express as px

# Substitua 'wines4' pelo nome real do seu DataFrame
# Certifique-se de que sua base de dados 'wines4' possui as colunas 'color' e 'quality'
fig = px.histogram(wines4, x='quality', color='color', barmode='group', title='Quantidade de Vinhos por Qualidade', labels={'quality': 'Qualidade'}, category_orders={'color': ['red', 'white']})

# Exibir o gráfico interativo
fig.show()

import plotly.figure_factory as ff
import plotly.graph_objects as go

# Substitua 'wines4' pelo nome real do seu DataFrame
# Certifique-se de que sua base de dados 'wines4' possui as colunas 'alcohol' e 'residual sugar'
wines4_red = wines4[wines4['color'] == 1]
wines4_white = wines4[wines4['color'] == 0]

fig1 = ff.create_2d_density(
    wines4_red['alcohol'],
    wines4_red['residual sugar'],
    colorscale='Viridis',
    hist_color='rgba(255, 0, 0, 0.7)',
    point_size=3
)

fig1.add_trace(
    go.Scatter(
        x=wines4_white['alcohol'],
        y=wines4_white['residual sugar'],
        mode='markers',
        marker=dict(color='rgba(0, 0, 255, 0.7)'),
        text='Açúcar: ' + wines4_white['residual sugar'].astype(str) +
             '<br>Álcool: ' + wines4_white['alcohol'].astype(str),
    )
)

fig1.update_layout(
    title='Relação entre Álcool e Açúcar',
    xaxis_title='Álcool',
    yaxis_title='Açúcar'
)

fig1.show()

"""3- Utilize um boxplot para verificar se existe algum vinho que seja considerado um outlier. Utilize x = 'quality' e y='residual sugar'. Identificando os outliers, crie um novo dataframe, utilize um barplot para contabilizar a quantidade de vinhos tintos e brancos por qualidade de modo que as barras estejam sobrepostas em relação ao tipo de vinho."""

fig_boxplot = px.box(wines4, x='quality', y='residual sugar', color='color', notched=True,
                     title='Boxplot de Residual Sugar por Qualidade e Tipo de Vinho',
                     labels={'quality': 'Qualidade', 'residual sugar': 'Residual Sugar'},
                     category_orders={'quality': sorted(wines4['quality'].unique())}
                    )

# Identificar outliers e criar um novo DataFrame
q1 = wines4.groupby('quality')['residual sugar'].quantile(0.25)
q3 = wines4.groupby('quality')['residual sugar'].quantile(0.75)
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

outliers = wines4[(wines4['residual sugar'] < lower_bound[wines4['quality']].values) | (wines4['residual sugar'] > upper_bound[wines4['quality']].values)]

# Criar barplot contabilizando a quantidade de vinhos tintos e brancos por qualidade
fig_barplot = px.bar(outliers, x='quality', color='color', barmode='group',
                     title='Quantidade de Vinhos Tintos e Brancos Considerados Outliers por Qualidade',
                     labels={'quality': 'Qualidade', 'count': 'Quantidade'},
                     category_orders={'quality': sorted(outliers['quality'].unique())}
                    )

# Exibir os gráficos interativos
fig_boxplot.show()
fig_barplot.show()

"""4- Faça um gráfico de correlação e encontre quais são as 'features' que contém correlações positivas e negativas fortes entre si. Em seguida, utilize o scatterplot, colocando no eixo "x" e "y" cada variável correlata e descreva por escrito o motivo da distribuição e o sentido vetorial estarem apresentados das respectivas formas.

ps: Para este problema, entenda como correlações fortes valores menores que -0.4 e maiores que 0.4.
"""

import plotly.express as px
import plotly.io as pio
from plotly.offline import init_notebook_mode

# Inicializar a renderização do Plotly no Colab
init_notebook_mode(connected=True)
pio.renderers.default = 'colab'

df = wines4
# Calcular a matriz de correlação
correlation_matrix = wines4.corr()

# Definir o número mínimo de pontos necessários para criar um gráfico
min_data_points = 20

# Encontrar pares de características com correlações positivas e negativas fortes
strong_positive_corr = (correlation_matrix > 0.4) & (correlation_matrix < 1)
strong_negative_corr = (correlation_matrix < -0.4) & (correlation_matrix > -1)

# Filtrar pares com correlações fortes e dados significativos
strong_positive_pairs = [
    (feature1, feature2)
    for feature1 in df.columns
    for feature2 in df.columns
    if (
        strong_positive_corr.loc[feature1, feature2]
        and not df[feature1].isna().any()
        and not df[feature2].isna().any()
        and df[[feature1, feature2]].count().min() >= min_data_points
    )
]

strong_negative_pairs = [
    (feature1, feature2)
    for feature1 in df.columns
    for feature2 in df.columns
    if (
        strong_negative_corr.loc[feature1, feature2]
        and not df[feature1].isna().any()
        and not df[feature2].isna().any()
        and df[[feature1, feature2]].count().min() >= min_data_points
    )
]

# Criar gráfico de dispersão apenas para pares com dados significativos
for pair in strong_positive_pairs:
    fig = px.scatter(df, x=pair[0], y=pair[1], title=f'Scatter Plot - {pair[0]} vs {pair[1]}')
    fig.show()

for pair in strong_negative_pairs:
    fig = px.scatter(df, x=pair[0], y=pair[1], title=f'Scatter Plot - {pair[0]} vs {pair[1]}')
    fig.show()

"""Alguns gráficos estão em branco pois as variáveis esolhidas para esses pares escolhidos podem ter uma baixa variabilidade ou não sobreposição de dados significativa.

# Modelos Supervisionados

### Classificação
**Utilize o dataset 'wine_classification.csv'.**

Após alguns meses, o filho do Sir. Donald Shelby, Chuck Norris Shelby, mais conhecido como "El Chavo del Ocho", em decorrência de seu "notório" saber e comportamento extrovertido, ~para ser eufemista~, foi promovido a "*head*" de Machine Learning, vulgo seu chefe.

Com suas inusitadas e inovadoras ideias, pediu que você criasse três modelos de árvores, um do tipo "random" e dois do tipo "boost", pois havia descoberto que a otimização pelo gradiente descendente era considerada como "*The American Dream*". Não obstante, gostaria de analisar o gráfico de importância das features.

Ademais, requereu que utilizasse o algoritmo SVM, pelo fato do "kernel trick" performar bem em problemas de altas dimensionalidades. Um KNN "cairia bem também, vamos utilizar por mero desencargo de consciência", disse.  

Em seu discurso inflamado, se pronunciava: "Precisamos realizar tais façanhas nunca vistas na história da Inteligência Artificial, desde que as redes neurais foram introduzidas por Walter Pitts e Warren McCulloch em 1943. Vamos predizer tudo que quisermos, independentemente da uva utilizada na produção. Eu transformarei nossa vinícola na melhor do mundo, pois sou detentor do saber". Tudo dito numa reunião contendo 12 pessoas, trabalhadores braçais inclusos. Tal discurso invejou os oráculos delfos e os lembraram de Sócrates em seu julgamento, antes de morrer.

Após tamanhas proclamações, apontou em sua direção e disse: **"VOCÊ, É..., VOCÊ MESMO**, irás fazer todo o processo por conta própria, e eu direi se o que fazes está correto! Não utilizarás Auto-ML, pois eu, ~professor~, quero ter certeza de que entende seu labor e suas nuâncias".

Você, cansado e entediado de tantas lorotas, se retirou da reunião com "dores" na região abdominal, porém ainda recebeu um e-mail lhe instruindo a comparar os resultados de todas as implementações, escolher o melhor modelo e utilizar métodos de otimização de hiperparâmetro.  

Em suma?

1- Crie um pipeline que contenha ao menos 05 tipos diferentes de algoritmos de classificação.

2- Crie um DataFrame que contenha todos os resultados de todos os algoritmos utilizados, inclusive a métrica ROC AUC.

3- Comparar os resultados, escolher o melhor modelo e otimizar os parâmetros. Ao fim, faça um gráfico da ROC AUC.

Fazer um for para avaliar o melhor parâmetro, assim como ele fez em sala no 1_3_knnimputer. imputer = KNNImputer(n_neighbors=5).set_output(transform='pandas')
iris = imputer.fit_transform(iris)
"""

winclas = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wine_classification.csv')

winclas.head()

winclas['quality'].value_counts()

X = winclas.drop(['quality', 'level_0','index','chlorides','free sulfur dioxide'], axis =1)

scaler = MinMaxScaler()

y = winclas.loc[:,'quality']

# Define SMOTE-Tomek Links
resample=SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))
X, y = resample.fit_resample(X, y)

class_mapeado = {3.0: 0, 4.0: 1, 5.0: 2, 6.0: 3, 7.0: 4, 8.0: 5}
winclas_mapeado = winclas
winclas_mapeado['quality'] = winclas_mapeado['quality'].map({3.0: 0, 4.0: 1, 5.0: 2, 6.0: 3, 7.0: 4, 8.0: 5})
X_mapeado = winclas_mapeado.drop(['quality', 'level_0','index','chlorides','free sulfur dioxide'], axis =1)
X_mapeado = scaler.fit_transform(X_mapeado)
y_mapeado = winclas_mapeado.loc[:,'quality']
resample=SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))
X_mapeado, y_mapeado = resample.fit_resample(X_mapeado, y_mapeado)
X_train_mapeado, X_test_mapeado, y_train_mapeado, y_test_mapeado = train_test_split(X_mapeado, y_mapeado, test_size=0.30, random_state=101)
X_train, X_test, y_train, y = X_train_mapeado, X_test_mapeado, y_train_mapeado, y_test_mapeado

scale_pos_weight = sum(y_train == 5) / sum(y_train)

y.value_counts()

winclas.isnull().sum()

"""Após todos seus esforços, o amado chefe lhe pediu para utilizar um algoritmo de classificação que ele ouviu falar, criado pelo matemático inglês Thomas Bayes. Cabe a você, mais uma vez aplicar o algoritmo e apresentar os resultado. Em seguida, faça uma breve explicação do principal problema desse método para solucionar problemas complexos.  """

ada = AdaBoostClassifier()
ada.fit(X_train,y_train)
ada_predict = ada.predict(X_test)

print(confusion_matrix(y_test_mapeado, ada_predict))
print('---------------------------\n')
print(classification_report(y_test_mapeado, ada_predict))

model = CatBoostClassifier(iterations=1000,
                           task_type="CPU",
                           devices='0:1',
                           depth=2,
                          )
catbooster = model.fit(X_train,y_train,verbose=0,plot=False)
catboosterpredict = catbooster.predict(X_test)

print(confusion_matrix(y_test_mapeado, catboosterpredict))
print('---------------------------\n')
print(classification_report(y_test_mapeado, catboosterpredict))

light = LGBMClassifier()
light.fit(X_train,y_train)
light_predict = light.predict(X_test)

print(confusion_matrix(y_test_mapeado, light_predict))
print('---------------------------\n')
print(classification_report(y_test_mapeado, light_predict))

gb=GradientBoostingClassifier()
gb.fit(X_train, y_train)
gb_predict = gb.predict(X_test)

print(confusion_matrix(y_test_mapeado, gb_predict))
print('---------------------------\n')
print(classification_report(y_test_mapeado, gb_predict))

xgb=XGBClassifier(device ='cuda',scale_pos_weight=scale_pos_weight)
xgb.fit(X_train_mapeado,y_train_mapeado)

# model = xgb.XGBClassifier(X_test,)
xgb_predict = xgb.predict(X_test_mapeado)
model.fit(X_train, y_train)

print(confusion_matrix(y_test_mapeado, xgb_predict))
print('---------------------------\n')
print(classification_report(y_test_mapeado, xgb_predict))

plot_importance(xgb)

models = [
    ('ada', AdaBoostClassifier()),
    ('light', LGBMClassifier(verbose=0)),
    ('xgb', XGBClassifier(device='cuda')),
    ('gb', GradientBoostingClassifier()),
    ('catboost', CatBoostClassifier())
]

for name, model in models:
    model_steps = Pipeline(steps=[('models', model)])
    model_steps.fit(X_train, y_train)
    predictions = model_steps.predict(X_test)
    print(f'-------------------------{name}--------------------------\n')
    print(confusion_matrix(y_test_mapeado, predictions))
    print(classification_report(y_test_mapeado, predictions))

y_test_mapeado

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, classification_report

# Assuming X_train, y_train, X_test, and y_test_mapeado are defined earlier

# XGBoost
xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_train)), use_label_encoder=False, device='cuda')
models = [
    ('ada', AdaBoostClassifier()),
    ('light', LGBMClassifier()),
    ('xgb', XGBClassifier(device='cuda',)),
    ('gb', GradientBoostingClassifier()),
    ('catboost', CatBoostClassifier())
]


# Binarize the labels for multi-class ROC curves
classes = np.unique(y_test_mapeado)
y_test_bin = label_binarize(y_test_mapeado, classes=classes)

for name, model in models:
    model_steps = Pipeline(steps=[('model', model)])
    model_steps.fit(X_train, y_train)
    predictions = model_steps.predict(X_test)

    # Get predicted probabilities for each class
    probas = model_steps.predict_proba(X_test)

    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], probas[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Plot ROC curve for each class
    plt.figure(figsize=(8, 8))
    for i in range(len(classes)):
        plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {classes[i]} (area = {roc_auc[i]:.2f})')

    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve for {name}')
    plt.legend(loc="lower right")
    plt.show()

    print(f'-------------------------{name}--------------------------\n')
    print(confusion_matrix(y_test_mapeado, predictions))
    print(classification_report(y_test_mapeado, predictions))

"""Dr. Anton Ego marcou uma data para comparecer na vinícula e degustar seus melhores rótulos. Nascido na França e o enólogo mais famoso do mundo, Anton era temido pelas suas análises minuciosas e certeiras. As críticas eram tão serveras que tão severas que 80% das vinículas eram fechadas pela falta de aceitação do mercado. Apenas =~ 19.99% sobreviviam sem danos consideráveis e somente $0.1x10^{-15}$% se tornavam uma lenda.

Chuck tomava leite da papoula para suportar tamanha disruptura emocional. Sir.Donald, tomado pela a ansiedade, estava com seus pruridos mentais em Nárnia até que sua esposa, Srta.Audrey Hepburn assumiu a liderança do projeto com a serenidade de um bebê.

Primeiramente ordenou que todos os vinhos que já vinham há algum tempo em processo de envelhecimento em barricas de carvalho fossem engarrafados e que amostras de todos fossem coletadas para análise.

Sua maior preocupação é que somente sejam servidos os vinhos de nota oito ou nove, pois ambos são de mesmíssima qualidade, ficando a avaliação a critério da subjetividade palatal do degustador. Em **<u>hipótese nenhuma</u>** um vinho que não tenha tais notas pode ser servido.

De todas as novas garrafas, serão servidas somente três que você autorizar. O Dr. Ego só toma vinho tinto!


Sabendo que você já tinha um modelo validado para solucionar este tipo de problema, pediu que o usasse na base **'desafio.csv''**. Ao fim, crie uma célula e copie os 3 vinhos que escolheu para registrar sua resposta.

"""

desafio = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/desafio.csv')

desafio

wines = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines.csv')

condition = (wines['color'] == 'white') | (wines['color'] == 'red')
wines = wines[condition]
wines['color'] = wines['color'].map({'white': 1, 'red': 0})

X = wines.drop(['quality','color','chlorides','free sulfur dioxide'], axis =1)

X = scaler.fit_transform(X)
y = wines.loc[:,'color']
resample=SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))
X, y = resample.fit_resample(X, y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)

y_train.value_counts()

models = [('ada',AdaBoostClassifier()),('light',LGBMClassifier(verbose=0 ,num_class = 1)),('xgb',XGBClassifier(device ='cuda')),('gb',GradientBoostingClassifier())]

for name, model in models:
    model_steps = Pipeline(steps=[('models', model)])
    model_steps.fit(X_train, y_train)
    predictions = model_steps.predict(X_test)
    print(f'-------------------------{name}--------------------------\n')
    print(confusion_matrix(y_test, predictions))
    print(classification_report(y_test, predictions))

preditos = xgb.predict(X)

# Encontrar índices onde o valor é 5
indices_5 = np.where(preditos == 5)[0]

# Selecionar valores e posições onde o valor é 5
valores_5 = preditos[indices_5]

X[indices_5]

indices_5

modelo = XGBClassifier(use_label_encoder=False)
modelo.fit(X_train, y_train)
vinh = modelo.predict(X[indices_5])

vinh

"""**O modelo previu apenas dois vinhos que podem ser servidos, criamos um algoritmo para verificar se o vinho era tinto ou não, os dois escolhidos são tintos. São eles os vinos com indices 1601 ee 8616**

**Utilize o dataset 'wines.csv' e 'wines_splines.csv'**

Chuck Norris tem um amigo famoso no mundo da ciência de dados, seu nome é Rocky Balboa. Em uma conversa sobre alguns métodos que podem ser utilizados para criar novos vetores (_features engineering_), o Sr.Rocky propôs que fossem utilizados Splines. Completamente emocionado com a ideia, Chuck decidiu aplicar esta técnica utilizando até a oitava potência.

Ele pediu a você que fizesse um estudo comparativo utilizando o PCA. O intuito é analisar se a redução de dimensionalidade pode ser vantajosa para o dataset original e o dataset com Splines.

1- Compare a variância explicada de cada um dos datasets

2- Explique porque o PCA seria, ou não uma boa abordagem para o dataset com Splines. Ademais, discorra sobre a influência de ruídos.

3- Utilize um loop "for" e crie uma condição para que, quando a variância for maior do que 0.92, seja retornado o número de features totais, faça para ambos datasets.
"""

#Carregando as bases de dados do problema
#chamei de wines5 para não confundir
wines5 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines.csv')
win_spline = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines_splines.csv')

#tirando as cores write e red e trocando por 0 e 1 respectivamente de wines5
mappings={'white':0,'red':1}
wines5['color']=wines5['color'].map(mappings)

# #tirando as cores de wines_splines
win_spline['color']=win_spline['color'].map(mappings)

#conferindo se em wines5 ficaram com cores 0 e 1
wines5.head()

#conferindo se em wines5 ficaram com cores 0 e 1
win_spline.head()

# Separar as features e as labels/ considerando que minha classe alvo é quality
X_original = wines5.drop('quality', axis=1)  # Substitua 'class' pelo nome da coluna de labels
X_splines = win_spline.drop('quality', axis=1)

X_original_scaled = scaler.fit_transform(X_original)
X_splines_scaled = scaler.fit_transform(X_splines)

# Aplicar PCA nos datasets originais e com Splines
pca_original = PCA().fit(X_original_scaled)
pca_splines = PCA().fit(X_splines_scaled)

# Calcular a variância explicada acumulada
var_exp_original = np.cumsum(pca_original.explained_variance_ratio_)
var_exp_splines = np.cumsum(pca_splines.explained_variance_ratio_)

# Plotar as curvas de variância explicada acumulada
plt.figure(figsize=(12, 6))
plt.plot(var_exp_original, label='Original Dataset')
plt.plot(var_exp_splines, label='Splines Dataset')
plt.title('Variância Explicada Acumulada por Número de Componentes Principais')
plt.xlabel('Número de Componentes Principais')
plt.ylabel('Variância Explicada Acumulada')
plt.legend()
plt.show()

"""Será gerado um gráfico com duas linhas uma azul que representa o conjunto de dados originais e a laranja para o conjunto de dados com Splines. As curvas começam a se estabilizar mostrando que 10 principais componentes são responsáveis pela definição da qualidade do vinho, e que com a Spline é adicionada mais complexidade ao conjunto de dados, exigindo mais componentes para representar a mesma informação. Assim a redução da dimensionalidade dada pelo uso da PCA será mais eficiente no conjunto de dados originais !

Com relação aos ruídos existem uma série de questões que podem ser abordadas:
1. Sensibilidade dos dados - Com as Splines de ordens mais altas o modelo pode ser mais sensível aos dados, modificando os polinômios em questão.
2. Supertreinamento (overliffiting)-
O uso de ordens mais altas pode levar a um supertreinamento do modelo, e ele ficar adequado apenas para a base de dados e tem dificuldade em generalizar principalmente se houverem ruídos ou variações nos dados
3. Interpretação-
Para a compreenssão do modelo o uso do spline pode piorar!
4. Alcance do modelo gerado-
as técnicas de PCA são projetados para lidar com padrões mais amplos, enquanto que as splines podem se ajustar aos detalhes dos daods
5. Validação-
Para a validação cruzada do modelo é interessante a utilização dos splines

3- Utilize um loop "for" e crie uma condição para que, quando a variância for maior do que 0.92, seja retornado o número de features totais, faça para ambos datasets.

Foram 8 componentes!
"""

# Separar as features e as labels (considerando 'quality' como a classe alvo)
X = wines5.drop('quality', axis=1)

# Normalizar os dados
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar PCA
pca = PCA()
pca.fit(X_scaled)

# Exibir os autovetores correspondentes aos primeiros 10 autovalores
num_componentes = 10
for i in range(num_componentes):
    print(f"Autovetor {i + 1}:\n{pca.components_[i]}\n")

def encontrar_numero_componentes(X_scaled, limiar_variancia):
    pca = PCA()
    pca.fit(X_scaled)
    var_exp_acumulada = np.cumsum(pca.explained_variance_ratio_)

    for i, var in enumerate(var_exp_acumulada):
        if var > limiar_variancia:
            return i + 1  # Adicionamos 1 para contar a partir de 1, não de 0

# Separar as features e as labels (considerando 'quality' como a classe alvo)
X = wines5.drop('quality', axis=1)

# Normalizar os dados
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Encontrar o número mínimo de componentes para atingir uma variância de 0.92
limiar_variancia = 0.92
num_componentes_wines5 = encontrar_numero_componentes(X_scaled, limiar_variancia)

print(f"Número mínimo de componentes para wines5: {num_componentes_wines5}")

#primeiro para wines_splines
def encontrar_numero_componentes(X_scaled, limiar_variancia):
    pca = PCA()
    pca.fit(X_scaled)
    var_exp_acumulada = np.cumsum(pca.explained_variance_ratio_)

    for i, var in enumerate(var_exp_acumulada):
        if var > limiar_variancia:
            return i + 1  # Adicionamos 1 para contar a partir de 1, não de 0

# Separar as features e as labels (considerando 'quality' como a classe alvo)
X = win_spline.drop('quality', axis=1)

# Normalizar os dados
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Encontrar o número mínimo de componentes para atingir uma variância de 0.92
limiar_variancia = 0.92
num_componentes_wines_splines = encontrar_numero_componentes(X_scaled, limiar_variancia)

print(f"Número mínimo de componentes para wines_splines: {num_componentes_wines_splines}")

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Carregando as bases de dados do problema
# Chamei de wines6 para não confundir
wines6 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines.csv')


# Tirando as cores write e red e trocando por 0 e 1 respectivamente de wines5
mappings = {'white': 0, 'red': 1}
wines6['color'] = wines6['color'].map(mappings)

# Conferindo se em wines6 ficaram com cores 0 e 1
wines6.head(500)

silhouette_scores = []

# Loop sobre diferentes valores de k
for k in range(2, 13):
    # Modelo K-means
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(wines6)

    # Cálculo da pontuação da silhueta
    silhouette_avg = silhouette_score(wines6, labels)
    silhouette_scores.append(silhouette_avg)

# Plote da curva da silhueta para a escolha do número ideal de clusters
plt.plot(range(2, 13), silhouette_scores, marker='o')
plt.title('Método da Silhueta para Escolha de K')
plt.xlabel('Número de Clusters (K)')
plt.ylabel('Pontuação da Silhueta')
plt.show()

"""**Utilize o dataset 'wines.csv'**

Uma ideia realmente interessante é a clusterização. Por vezes, podemos nos espantar com certos resultados. Aqui, você deve utilizar o dataset original e separar cada nota em um cluster.

1- Validar os resultados do algoritmo Kmeans com o dataset original

2- Aplicar o método do Cotovelo e averiguar se o número de clusters apontados são iguais ao número de cluster que você tem de usar.

3- Utilizar o método da Silhueta e averiguar se o número de clusters apontados são iguais ao número de cluster que você tem de usar.

4- Explique os principais conceitos dos métodos das questões 2 e 3.
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings as warn
warn.filterwarnings('ignore')
import pandas as pd
import numpy as np
from sklearn import model_selection, cluster
import matplotlib.pyplot as plt
from matplotlib import style
from time import time
import math
style.use('ggplot')
# %matplotlib inline

def test_cluster(clf, data):

    t0 = time()
    clf.fit(data)
    centroids = clf.cluster_centers_
    print("Tempo:", round(time()-t0, 3), "s")

    colors = 10*["g","r","c","b","k"]

    for color, feature in zip(clf.labels_, data.values):
        plt.scatter(feature[6], feature[7], color=colors[color], s=10)


    for c in range(len(clf.cluster_centers_)):
        plt.scatter(centroids[c][6], centroids[c][7],
            marker="o", color=colors[c], edgecolors='w', s=50, linewidths=1)

    labels = data.columns
    plt.xlabel(labels[6])
    plt.ylabel(labels[7])
    plt.show()

# implementação própria para aprendizado do algoritmo
class k_means(object):

    # construtor
    def __init__(self, k, tol = 0.0001, max_iter = 500):
        self.k = k
        self.tol = tol
        self.max_iter = max_iter
        self.inertia_ = 0

    # Inertia: Soma das distâncias das amostras ao centroide mais próximo
    def inertia(self, data):
        feature_matrix={}
        for i in range(self.k) :
            feature_matrix[i]=[]

        centroids = []
        for l in self.cluster_centers_:
            if ~np.isnan(l[0]):
                centroids.append(l)

        for color, feature in zip(self.labels_, data.values):
            feature_matrix[color].append(feature)

        sum_ = 0

        for i in feature_matrix:
            try:
                pointcentroid=centroids[i]
            except:
                pointcentroid = np.NaN

            try:
                sum_ = sum_ + np.sum((feature_matrix[i] - pointcentroid)**2) #implementação de inércia, conforme indicado nos documentos do scikit, ou seja, soma da distância ao quadrado.
            except:
                sum_ = sum_ + 0

        self.inertia_ = sum_

    # treinamento

    def fit(self, data):
        self.soma=0
        self.labels_ = np.zeros(shape=(len(data)))
        # seleciona os centros aleatoriamente para começar
        rand_k = [np.random.randint(0, len(data)) for rand in range(self.k)]

        # definir aleatoriamente em qual cluster ficará um dado
        self.cluster_centers_ = data.iloc[rand_k, :].values

        # laço irá parar quando atingir max_iter ou atingir o limiar
        for _ in range(self.max_iter):
            # cria classes vazias para serem povoadas
            temp_class = {}
            for i in range(self.k):
                temp_class[i] = []

            # acha que ponto pertence a que centro / centroide
            for j, i in enumerate(data.values):
                # acha a distância entre a observação i e cada centro
                dist = [np.linalg.norm(i-kj) ** 2 for kj in self.cluster_centers_]
                # classifica a observação i a um centro
                clas = dist.index(min(dist))
                temp_class[clas].append(i)
                self.labels_[j] = clas

            # tira centro antigo para verificar otimização.
            # passa por cópia e ñ por referência
            prev_centers = np.array(self.cluster_centers_)

            # atualiza os centros
            for i, _ in enumerate(self.cluster_centers_):
                self.cluster_centers_[i] = np.array(temp_class[i]).mean(axis=0)

            # verifica convergência
            var = np.sum((self.cluster_centers_ - prev_centers) /
                         prev_centers*100.0)

            if var < self.tol:
                break  # sairá do laço, pois a diferença do
                       # cluster com cluster anterior é menor que o limiar definido

        # converta as classes para ints
        self.labels_ = self.labels_.astype(int)

        #calcular inertia_
        self.inertia(data)

import numpy as np
from joblib import Parallel, delayed
from sklearn.utils import gen_batches

#lê os dados
#dados podem ser encontrados no link abaixo
# http://www.cengage.com/aise/economics/wooldridge_3e_datasets/
# OBS: dados utilizados aqui já foram convertidos de excel para csv
win5 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/wines.csv')
mappings={'white':0,'red':1}
win5['color']=win5['color'].map(mappings)

#data = pd.read_csv('hprice.csv', sep=',', usecols = ['price', 'sqrft'])
#data.fillna(-99999, inplace = True)

# Testa regressor criado manualmente
print('\nResultados do criado manualmente')
clf = k_means(k=6)
clf.fit(win5)
print(clf.cluster_centers_)
test_cluster(clf, win5)

# Compara o regressor com o do sklearn
print('\nComparando com os resultados do Sklearn')
kmeans = cluster.KMeans(n_clusters=6)
kmeans.fit(win5)
print(clf.cluster_centers_)
test_cluster(clf, win5)
y_kmeans = kmeans.predict(win5)

test_cluster(clf, win5)

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import numpy as np

sse = {}
faixa = range(1,20)
for kclusters in faixa:
    kmeans1 = KMeans(n_clusters=kclusters)
    kmeans1.fit(win5)
    sse[kclusters] = kmeans1.inertia_ # Inertia: Soma das distâncias das amostras ao centro de aglomerado mais próximo

plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Número de cluster")
plt.ylabel("soma dos erros ao quadrado - SSE")
plt.title("Número de cluster por SSE - Algoritmo KMeans no SKLearn")
plt.show()

def elbow(array):
    se = []
    for n in range(2, 21):
        kmeans = KMeans(n_clusters=n)
        kmeans.fit(X=array)
        se.append(kmeans.inertia_)
    x1, y1 = 2, se[0]
    x2, y2 = 20, se[len(se)-1]
    distances = []
    for i in range(len(se)):
        x0 = i+2
        y0 = se[i]
        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)
        denominator = math.sqrt((y2 - y1)**2 + (x2 - x1)**2)
        distances.append(numerator/denominator)
    return distances.index(max(distances)) + 2

elbow(win5)

"""**QUESTÃO TEÓRICA:**

---



Ambos os métodos, do cotovelo e silhueta são técnicas matemáticas utilizadas na avaliação de algoritmos de agrupamento, como o Kmeans. O método do cotovelo é feito em relação à soma dos erros ao quadrado (WCSS), ele retorna um gráfico onde vamos procurar o valor ótimo de k, ou seja, é específico para ajudar a escolher o número ideal de clusters.  O ponto onde a curva começa a se assemelhar a um cotovelo é um candidato para o número ideal de clusters.
O método Silhueta é utilizado para avaliar a qualidade geral dos clusters. A pontuação da silhueta oferece uma métrica mais abrangente para avaliar a qualidade dos clusters formados, independente do número de clusters escolhido e, é útil para avaliar a coesão e separação dos clusters.
Em muitos casos, essas duas abordagens podem ser usadas em conjunto para uma avaliação mais completa.

**Utilize o dataset 'logs_firewall.xlsx'**

A Università di Bologna tem cursos de graduação e pós graduação em enologia. Os grandes enólogos do mundo são os únicos que podem fazer o doutorado nesta renomada universidade. Esta instituição tem um contrato milionário com o Sr.Donald, onde todos os alunos poderiam comparecer uma semana a cada três meses para estudar as características, mecânicas de plantações, tecnologias, processos de confecção dos vinhos, entre outras matérias. Todos os professores, escanções extremamente bem conceituados, sempre estão presentes.

Caso infortúnio, O Sr.Hafþór Júlíus Björnsson, mais conhecido como "o Montanha", chefe de segurança cibernética da empresa apertou o botão DEFCON-1 ao perceber que os servidores tinham sido 'hackeados'. Momento em que notou que os bancos de dados que continham as notas dos vinhos haviam sido alterados/deletados e o backup infectado por um Ransomware chamado "HUE HUE HUE BR". Aparentemente, os black-hats conseguiram alterar de 5% a 25% dos dados referentes aos vinhos tintos, antes que o Montanha conseguisse exterminar as conexões dos servidores.

O Sr.Donald Shelby aproximou-se para falar com você sobre as políticas da empresa, criadas por sua esposa, que dispunham sobre o bem estar, ambiente não tóxico, agregação dos "colaboradores" ~pseudo escravos~ como familiares, dentre outros ideais da mesma seara. Em seguida incorporou o espírito de Don Corleone e proferiu uma de suas máximas ao falar "Política é saber a hora de puxar o gatilho".   

Uma regra clara da empresa dispõe sobre a impossibilidade de extrair datasets como arquivos e, toda vez que for utilizar os dados no Jupyter Notebook, deve ser realizado uma query no datalake. Ocorre que, 'sem querer querendo', você estava "desatento" e salvou os dados para estudos quando estivesse em casa. Nítido que se disse-se que havia copiado quaisquer dados seria torturado, por isso não poderia simplesmente colocá-los de volta no banco e, como não queria morrer, tinha de encontrar vias oblíquas para dirimir a questão.

Erick Cartman, analista de infraestrutura, recebeu ordens para recuperar os dados a qualquer custo e, caso falhasse, seria devidamente penalizado ~executado~. Desolado, regado a fanta uva, com palavras arrastadas e chiadas, Erick lhe pediu ajuda. Com muita pena, pegou seu disquete que continha a cópia dos dados e o entregou, pedindo extrema confidencialidade.

Para sua surpresa, após ter a vida salva, receber aumentos salariais e bonificações, Erick te chantageou. "Agora pediram para eu analisar os logs do firewall que contém informações de acesso a servidores e descobrir os possíveis culpados. Eu não sei fazer isso não, 'ocê tá LOUKO'. Dá teus pulos aí se não eu te conto que você copia dados da empresa!!!"

Conhecedor de diversas técnicas para detecção de outliers, se lembrou de uma que já tinha experiência: Isolation Forest.

1- Descubra o nome do responsável pelo o ataque.

**ps**: Recentemente houve um estudo estatístico que comprovou que existe um fator de risco em relação aos horários de acesso, são eles:  

Entre 09:00 às 12:00 o risco varia entre 0-10%

Entre 12:00 às 14:00 o risco varia entre 5-10%

Entre 14:00 às 19:00 o risco varia entre 0-10%

Entre 19:00 às 21:00 o risco varia entre 20-30%

Entre 21:00 às 23:00 o risco varia entre 40-50%

Entre 00:00 às 02:00 o risco varia entre 60-80%

Entre 02:00 às 06:00 o risco varia entre 80-100%

Entre 06:00 às 09:00 o risco varia entre 30-40%

**<u>Esse fator pode ser recriado usando o método 'random.uniform', com 'seed' = 64.</u>**
"""

import pandas as pd
 hack = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/aulas_pos/andre/logs_firewall.xlsx')

hack.head(200)

hack['H'].value_counts()

label_encoder = LabelEncoder()

hack['maquina_encoded'] = label_encoder.fit_transform(hack['Maquina'])

hack['ip_encoded'] = label_encoder.fit_transform(hack['IP'])

hack['nome_encoded'] = label_encoder.fit_transform(hack['Nome'])

h_mapping = {
    6: 3, 7: 3, 8: 3,
    9: 0, 10: 0, 11: 0,
    12: 1, 13: 1, 14: 0,
    15: 0, 16: 0, 17: 0,
    18: 0, 19: 2, 20: 2,
    21: 4, 22: 4, 23: 5,
    0: 6, 1: 6, 2: 7,
    3: 7, 4: 7, 5: 7
}
hack['h_encoded'] = hack['H'].map(h_mapping)

hack_novo = hack.iloc[:, -4:].copy()

hack_novo

scaler = StandardScaler()

sns.heatmap(hack_novo.corr(numeric_only=True),annot=True)

isolation = IForest(contamination=0.01, n_jobs=-1)

isolation.fit(hack_novo)

previsoes = isolation.predict(hack_novo)
hack_novo['outlier'] = previsoes

outliers_iguais_a_1 = hack_novo[hack_novo['outlier'] == 1]
print(outliers_iguais_a_1)

hack.loc[hack_novo['outlier'] == 1]

# FOI O MAGAIVER